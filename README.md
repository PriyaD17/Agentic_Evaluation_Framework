# Agentic Evaluation Framework - e6data Hackathon

This project is a complete, end-to-end framework for evaluating the performance of AI agents across multiple, nuanced dimensions. It goes beyond simple "accuracy" to measure "helpfulness" and provides AI-powered justifications for its scores.
The framework is built to be scalable, explainable, and versatile, capable of handling diverse evaluation tasks from simple Q&A to complex reasoning.

![alt text](leaderboard.png)

(This chart is automatically generated by the script)

## The Problem
Manually evaluating AI agents is impossible at scale. Furthermore, a simple accuracy score is insufficient. An agent's response can be:
- Factually correct, but fail to follow instructions.
- Creative and helpful, but not a textbook-perfect answer.
- Logically flawed, even if it seems plausible.

We need an automated system that can capture these nuances to truly understand agent performance.
## The Solution
Built a multi-dimensional evaluation pipeline that scores agents on two key axes:
1.  **Rule-Based Instruction Following:** A precise, code-based check to see if the agent followed specific constraints (e.g., "respond in a single sentence").
2.  **AI-Based Quality Scoring:** Using Gemini 1.5 Pro as an "AI Judge," the framework evaluates the **helpfulness** and **relevance** of the response from a user's perspective.

### Key Features
*   **Explainability:** The AI Judge provides a one-sentence justification for every score, explaining *why* a response was deemed helpful or unhelpful.
*   **Scalability:** The pipeline reads data from an external `.csv` file, allowing it to evaluate thousands of responses.
*   **Multi-Domain Support:** The framework was successfully tested on Question-Answering, Summarization, and Logical Reasoning (Multi-step logic problems) tasks.
*   **Automated Leaderboard & Visualization:** The script generates a final text-based leaderboard and a visual bar chart (`leaderboard.png`) for an at-a-glance understanding of agent performance.

## How to Run It
1.  **Prerequisites:** Python 3.7+
2.  **Install dependencies:**
    ```bash
    pip install google-generativeai python-dotenv matplotlib
    ```
3.  **Set up your API Key:**
    *   Create a `.env` file in the root directory.
    *   Add your Google AI API key: `GEMINI_API_KEY="YOUR_KEY_HERE"`
4.  **Run the pipeline:**
    ```bash
    python evaluate.py
    ```

The script will print detailed reports for each response and generate the leaderboard.png chart upon completion.

## Project Structure
```code
├── evaluate.py         # The main evaluation pipeline script
├── data.csv            # Sample input data for the agents
├── requirements.txt    # Project dependencies
├── .env                # Your secret API key (ignored by git)
├── .gitignore          # Tells git to ignore .env and other files
└── README.md           # You are here!
```

## Accomplishments & Issues Faced
During this hackathon, the entire pipeline was built from scratch. A key challenge was evolving the evaluation metric from simple "accuracy" to "helpfulness" after discovering that powerful AI judges are very literal. By refining the prompts and reengineering the logic, a more robust and insightful system was created by evaluating the "Helpfulness"